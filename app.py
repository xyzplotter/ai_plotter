import streamlit as st
import openai
from io import BytesIO
import requests
from PIL import Image, ImageOps
from streamlit_mic_recorder import mic_recorder
import cv2
import numpy as np

# ==========================================
# ğŸ” API í‚¤ ì„¤ì • (OpenAI í•˜ë‚˜ë¡œ í†µì¼!)
# ==========================================
try:
    # secrets.tomlì— ìˆëŠ” OPENAI_API_KEY ì‚¬ìš©
    openai.api_key = st.secrets["OPENAI_API_KEY"]
    client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
except Exception as e:
    st.error(f"OpenAI API í‚¤ ì˜¤ë¥˜! secrets.tomlì„ í™•ì¸í•´ì£¼ì„¸ìš”.\nì—ëŸ¬: {e}")
    st.stop()

# ==========================================
# ğŸ§  [í†µì—­ì‚¬] GPT-4o-mini (ë²ˆì—­ ë‹´ë‹¹)
# ==========================================
def translate_to_english_gpt(text):
    try:
        # [í•µì‹¬ ìˆ˜ì •] í•œêµ­ì–´ëŠ” ë²ˆì—­í•˜ê³ , ì˜ì–´ëŠ” ë‹¤ë“¬ì–´ì£¼ëŠ” ë˜‘ë˜‘í•œ í”„ë¡¬í”„íŠ¸ ì ìš©!
        system_prompt = """You are an expert prompt engineer for DALL-E. 
        Your task is to convert user input into a descriptive English prompt for image generation.
        1. If the input is in Korean, translate it accurately into English.
        2. If the input is already in English, refine it to be more descriptive for DALL-E.
        Output ONLY the final English prompt string."""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error: {e}"

# ==========================================
# ğŸ¤ [ê·€] Whisper-1 (ìŒì„± ì¸ì‹ ë‹´ë‹¹)
# ==========================================
def transcribe_audio_whisper(audio_bytes):
    try:
        audio_file = BytesIO(audio_bytes)
        audio_file.name = "voice.wav"
        
        # [ìœ ì§€] í•œêµ­ì–´ ì¸ì‹ë¥  ìµœìš°ì„ ì„ ìœ„í•´ language="ko" ê³ ì •
        # ì˜ì–´ë¥¼ ë§í•´ë„ Whisperê°€ ì•Œì•„ì„œ í•œêµ­ì–´ë¡œ ìŒì°¨( transliteration)í•˜ê±°ë‚˜
        # ì‰¬ìš´ ë‹¨ì–´ëŠ” ì˜ì–´ë¡œ ì ì–´ì£¼ëŠ”ë°, ì´ê±¸ ìœ„ì—ì„œ GPTê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•¨.
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language="ko"
        )
        return transcript.text.strip()
    except Exception as e:
        st.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
        return ""

# ==========================================
# ğŸ“¸ [ì†] DALL-E 2 (ê·¸ë¦¼ ë‹´ë‹¹)
# ==========================================
def generate_dalle_image(english_prompt):
    try:
        # ì‹¤ì‚¬ ëŠë‚Œì„ ê°•ì¡°í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        full_prompt = f"{english_prompt}, photorealistic photograph, detailed, sharp focus, white background."
        response = client.images.generate(
            model="dall-e-2",
            prompt=full_prompt,
            size="1024x1024",
            n=1
        )
        return response.data[0].url
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# ==========================================
# ğŸ¨ [ë³€í™˜ê¸°] ì´ë¯¸ì§€ -> ìŠ¤ì¼€ì¹˜ (OpenCV)
# ==========================================
def convert_to_sketch(image_bytes):
    # 1. ì´ë¯¸ì§€ ë¡œë“œ
    file_bytes = np.asarray(bytearray(image_bytes), dtype=np.uint8)
    image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    
    # 2. í‘ë°± ë³€í™˜ ë° ë¸”ëŸ¬ (ë…¸ì´ì¦ˆ ì œê±°)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    
    # 3. Canny Edge Detection (ì™¸ê³½ì„  ê²€ì¶œ)
    # threshold1, 2 ê°’ì„ ì¡°ì ˆí•˜ë©´ ì„ ì˜ ë””í…Œì¼ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. (í˜„ì¬ 50, 150)
    edges = cv2.Canny(blurred_image, 50, 150)
    
    # 4. ìƒ‰ìƒ ë°˜ì „ (ê²€ì€ ë°°ê²½ í° ì„  -> í° ë°°ê²½ ê²€ì€ ì„  - í”Œë¡œí„°ìš©)
    inverted_edges = cv2.bitwise_not(edges)
    
    # 5. ê²°ê³¼ ë°˜í™˜
    is_success, buffer = cv2.imencode(".png", inverted_edges)
    return buffer.tobytes()

# ==========================================
# ğŸ–¥ï¸ ë©”ì¸ UI
# ==========================================
st.set_page_config(page_title="AI Plotter Controller", page_icon="ğŸ¤–")
st.title("ğŸ¤– AI í”Œë¡œí„° ì»¨íŠ¸ë¡¤ëŸ¬ (í†µí•© ë²„ì „)")
st.caption("OpenAI(ìŒì„±/ë²ˆì—­/ê·¸ë¦¼) + OpenCV(ìŠ¤ì¼€ì¹˜ ë³€í™˜) ì—”ì§„ íƒ‘ì¬")

st.divider()

# --- 1. ìŒì„± ì…ë ¥ ---
c1, c2 = st.columns([1, 4])
with c1:
    st.write("ğŸ¤ ìŒì„± ëª…ë ¹:")
with c2:
    audio = mic_recorder(start_prompt="ğŸ”´ ë…¹ìŒ ì‹œì‘", stop_prompt="â¹ ë…¹ìŒ ì¢…ë£Œ", just_once=True, key='rec')

if 'voice_msg' not in st.session_state:
    st.session_state.voice_msg = ""

if audio:
    with st.spinner("Whisperê°€ ë“£ëŠ” ì¤‘..."):
        st.session_state.voice_msg = transcribe_audio_whisper(audio['bytes'])

user_prompt = st.text_input("ì£¼ì œ ì…ë ¥ (í•œê¸€/ì˜ì–´):", value=st.session_state.voice_msg)

st.divider()

# --- 2. ì´ë¯¸ì§€ ìƒì„± ë²„íŠ¼ ---
if st.button("ğŸ“¸ ì‹¤ì‚¬ ì´ë¯¸ì§€ ìƒì„±í•˜ê¸° (DALL-E 2)", type="primary", use_container_width=True):
    if not user_prompt:
        st.warning("ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    else:
        # 1ë‹¨ê³„: ë²ˆì—­ (GPT-4o-mini)
        with st.spinner("GPTê°€ DALL-Eë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„± ì¤‘..."):
            english_prompt = translate_to_english_gpt(user_prompt)
        
        if english_prompt.startswith("Error"):
            st.error(f"ğŸ›‘ í”„ë¡¬í”„íŠ¸ ì‘ì„± ì‹¤íŒ¨: {english_prompt}")
        else:
            st.info(f"ğŸ”¤ DALL-E í”„ë¡¬í”„íŠ¸: **[{english_prompt}]**")

            # 2ë‹¨ê³„: ê·¸ë¦¼ (DALL-E 2)
            with st.spinner(f"DALL-E 2ê°€ ê·¸ë¦¬ëŠ” ì¤‘... (ì•½ 20ì›)"):
                img_url = generate_dalle_image(english_prompt)
                
                if img_url:
                    img_data = requests.get(img_url).content
                    st.session_state.generated_image = img_data
                    # ìƒˆ ê·¸ë¦¼ ìƒì„± ì‹œ ê¸°ì¡´ ë³€í™˜ ê²°ê³¼ ì´ˆê¸°í™”
                    if 'processed_image' in st.session_state:
                        del st.session_state.processed_image
                    st.success("ìƒì„± ì™„ë£Œ!")

# --- 3. ê²°ê³¼ í™•ì¸ ë° ë³€í™˜ ---
if 'generated_image' in st.session_state:
    st.image(st.session_state.generated_image, caption="ì›ë³¸ ì‹¤ì‚¬ ì´ë¯¸ì§€", use_container_width=True)
    
    st.divider()
    st.subheader("ğŸ¨ í”Œë¡œí„°ìš© ë³€í™˜ ìŠ¤íƒ€ì¼ ì„ íƒ")
    
    b1, b2, b3 = st.columns(3)
    
    with b1:
        if st.button("ğŸ“ ì§€ì˜¤ë©”íŠ¸ë¦­", use_container_width=True):
            st.info("ğŸš§ ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì˜ˆì • (Next Step!)")
            
    with b2:
        if st.button("ã€°ï¸ ì›ë¼ì¸", use_container_width=True):
             st.info("ğŸš§ ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì˜ˆì • (Next Step!)")
            
    with b3:
        # OpenCV ìŠ¤ì¼€ì¹˜ ë²„íŠ¼
        if st.button("ğŸ–Šï¸ ìŠ¤ì¼€ì¹˜ (Edge)", type="secondary", use_container_width=True):
            with st.spinner("OpenCVê°€ ì™¸ê³½ì„ ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                processed_data = convert_to_sketch(st.session_state.generated_image)
                st.session_state.processed_image = processed_data
                st.toast("âœ… ìŠ¤ì¼€ì¹˜ ë³€í™˜ ì™„ë£Œ!")

    # ë³€í™˜ëœ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    if 'processed_image' in st.session_state:
        st.divider()
        st.subheader("ğŸ–¨ï¸ í”Œë¡œí„° ì¶œë ¥ ê²°ê³¼ë¬¼ (Preview)")
        st.image(st.session_state.processed_image, caption="ìµœì¢… ë³€í™˜ ê²°ê³¼ (Canny Edge)", use_container_width=True)